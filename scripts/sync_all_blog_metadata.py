#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒæ­¥æ‰€æœ‰åšå®¢çš„æ ‡é¢˜ã€ç®€ä»‹ã€å…³é”®è¯è„šæœ¬
ä»HTMLæ–‡ä»¶ä¸­è¯»å–å½“å‰çš„æ ‡é¢˜ã€ç®€ä»‹ã€å…³é”®è¯ï¼ŒåŒæ­¥æ›´æ–°æ‰€æœ‰ç›¸å…³çš„metaæ ‡ç­¾
å¹¶è‡ªåŠ¨ä¸ºä¸­æ–‡æ ‡é¢˜ç”Ÿæˆè‹±æ–‡æ ‡é¢˜ï¼ˆé™¤äº†æ’é™¤çš„åšå®¢ï¼‰
"""

import re
import json
from pathlib import Path
from typing import Dict, Optional, Tuple
import html

# é…ç½®è·¯å¾„
SCRIPT_DIR = Path(__file__).parent
SITE_ROOT = SCRIPT_DIR.parent
BLOGS_DIR = SITE_ROOT / 'blogs'
TITLE_TRANSLATION_FILE = SCRIPT_DIR / 'title_translation_mapping.json'

# æ’é™¤çš„åšå®¢ï¼ˆä¸è¿›è¡Œè‹±æ–‡æ ‡é¢˜è½¬æ¢ï¼‰
EXCLUDED_BLOGS = [
    "ç­‘å±…æ€ï¼š37å²ï¼Œæˆ‘ç»ˆäºå­¦ä¼šäº†\"å®‰å¿ƒå»ç©\"",
    "37å²ï¼Œæˆ‘ç»ˆäºå­¦ä¼šäº†\"å®‰å¿ƒå»ç©\"",
    "learned-to-play-at-37",
    "2025-11-14-learned-to-play-at-37.html"
]


def load_title_translation() -> Dict[str, str]:
    """åŠ è½½ä¸­æ–‡æ ‡é¢˜åˆ°è‹±æ–‡æ ‡é¢˜çš„ç¿»è¯‘æ˜ å°„"""
    if TITLE_TRANSLATION_FILE.exists():
        with open(TITLE_TRANSLATION_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def save_title_translation(mapping: Dict[str, str]):
    """ä¿å­˜ä¸­æ–‡æ ‡é¢˜åˆ°è‹±æ–‡æ ‡é¢˜çš„ç¿»è¯‘æ˜ å°„"""
    with open(TITLE_TRANSLATION_FILE, 'w', encoding='utf-8') as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)


def translate_chinese_title(chinese_title: str, translation_mapping: Dict[str, str]) -> str:
    """
    å°†ä¸­æ–‡æ ‡é¢˜è½¬æ¢ä¸ºè‹±æ–‡æ ‡é¢˜
    ä¼˜å…ˆä½¿ç”¨æ˜ å°„è¡¨ï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•è‡ªåŠ¨ç¿»è¯‘
    
    æ ‡é¢˜æ ¼å¼ï¼šç­‘å±…æ€Â·[åˆ†ç±»]ï¼š[æ ‡é¢˜å†…å®¹]
    ä¾‹å¦‚ï¼šç­‘å±…æ€Â·åŸºå‡†ï¼šä¸€ä»½2022å¹´çš„"çµé­‚å¿«ç…§"ï¼ˆæ™®é²æ–¯ç‰¹é—®å·ï¼‰
    """
    # æ ‡å‡†åŒ–æ ‡é¢˜ï¼ˆç»Ÿä¸€å¼•å·æ ¼å¼ï¼Œä¾¿äºåŒ¹é…ï¼‰
    normalized_title = chinese_title.replace('"', '"').replace('"', '"').replace(''', "'").replace(''', "'")
    
    # å¦‚æœå·²ç»åœ¨æ˜ å°„è¡¨ä¸­ï¼Œç›´æ¥è¿”å›
    if chinese_title in translation_mapping:
        return translation_mapping[chinese_title]
    if normalized_title in translation_mapping:
        return translation_mapping[normalized_title]
    
    # æå–"ç­‘å±…æ€Â·[åˆ†ç±»]ï¼š"æ ¼å¼
    # ä¾‹å¦‚ï¼š"ç­‘å±…æ€Â·åŸºå‡†ï¼š" -> "åŸºå‡†"
    category_match = re.match(r'ç­‘å±…æ€Â·([^ï¼š:]+)[ï¼š:]', chinese_title)
    category = category_match.group(1) if category_match else None
    
    # ç§»é™¤"ç­‘å±…æ€ï¼š"æˆ–"ç­‘å±…æ€Â·[åˆ†ç±»]ï¼š"å‰ç¼€ï¼Œä¿ç•™æ ‡é¢˜å†…å®¹
    clean_title = chinese_title.replace('ç­‘å±…æ€ï¼š', '').replace('ç­‘å±…æ€Â·', '').strip()
    if category:
        clean_title = re.sub(rf'^{re.escape(category)}[ï¼š:]\s*', '', clean_title)
    clean_normalized = normalized_title.replace('ç­‘å±…æ€ï¼š', '').replace('ç­‘å±…æ€Â·', '').strip()
    if category:
        clean_normalized = re.sub(rf'^{re.escape(category)}[ï¼š:]\s*', '', clean_normalized)
    
    # åˆ†ç±»ç¿»è¯‘æ˜ å°„ï¼ˆ"ç­‘å±…æ€Â·[åˆ†ç±»]ï¼š"æ ¼å¼ï¼‰
    category_translations = {
        'åŸºå‡†': 'Baseline',
        'å›å“': 'Echo',
        'ç¼˜èµ·': 'Origin',
        'ç®—æ³•': 'Algorithm',
        'åˆŠç‰©': 'Newsletter',
        'å“²æ€': 'Philosophy',
        'Vibe': 'Vibe',
        'æˆé•¿': 'Growth',
        'ä¿®è¡Œ': 'Practice',
        'å®è·µ': 'Practice',
    }
    
    # å¸¸è§è¯æ±‡æ˜ å°„ï¼ˆç”¨äºç®€å•ç¿»è¯‘ï¼‰
    common_translations = {
        # å®Œæ•´æ ‡é¢˜åŒ¹é…
        'å†™åœ¨19å²': 'Writing to 19-Year-Old Self',
        'ç­‘å±…æ€Â·ç¼˜èµ·ï¼šæˆ‘çš„æ€æƒ³å¯è’™ä¸"çµé­‚æ –å±…"': 'Origin: My Intellectual Enlightenment and "Soul Dwelling"',
        'ç­‘å±…æ€Â·ç®—æ³•ï¼šä¸€ä¸ª"è›°ä¼"è€…çš„"é˜…è¯»é¡ºåº"': 'Algorithm: A "Hibernator\'s" Reading Order',
        'ç­‘å±…æ€Â·åˆŠç‰© (No.01)ï¼šæˆ‘çš„"å¿ƒæµ"å·¥å…·ç®±ä¸"æ•ˆç‡"å®éªŒ': 'Newsletter (No.01): My "Flow" Toolkit and "Efficiency" Experiment',
        'ç­‘å±…æ€ï¼šæˆ‘ä»KKçš„103æ¡å¿ å‘Šä¸­ï¼Œé‡æ„äº†æˆ‘çš„"äººç”Ÿç®—æ³•"': 'Reconstructing My "Life Algorithm" from KK\'s 103 Pieces of Advice',
        'ç­‘å±…æ€Â·å“²æ€ï¼šä»¥äººæ–‡ä¸»ä¹‰ä¸ºçƒ›å…‰ï¼Œå¯¹æŠ—çµé­‚çš„"ç†µå¢"': 'Philosophy: Using Humanism as Candlelight Against the "Entropy Increase" of the Soul',
        'ç­‘å±…æ€Â·å“²æ€ï¼šæˆ‘æ— æ³•ç”¨åˆ«äººçš„ç­”æ¡ˆï¼Œå›åº”æˆ‘çš„äººç”Ÿ': 'Philosophy: I Cannot Respond to My Life with Others\' Answers',
        'å¥½ä¹…ä¸è§ï¼Œæœ€è¿‘åœ¨å¤–å¤ªç©ºç§ä¸‹äº†å°èŠ±': 'Hello Again, Little Flowers in Space',
        'ç­‘å±…æ€Â·Vibeï¼šæˆ‘çš„äººæ–‡ã€ç§‘æŠ€ä¸"ç™½æ—¥æ¢¦"': 'Vibe: My Humanities, Technology, and "Daydreams"',
        'ç­‘å±…æ€Â·å“²æ€ï¼šä½ æ˜¯åœ¨"è¿‡ç”Ÿæ´»"ï¼Œè¿˜æ˜¯åœ¨"è®¡åˆ’ä½ çš„ä¼ è®°"ï¼Ÿ': 'Philosophy: Are You "Living Life" or "Planning Your Biography"?',
        'ç­‘å±…æ€Â·ç®—æ³•ï¼šé‡æ„"å†³ç­–"çš„38ä¸ªçµé­‚æ‹·é—®': 'Algorithm: 38 Soul-Searching Questions to Reconstruct "Decision-Making"',
        'ç­‘å±…æ€Â·æˆé•¿ï¼š"Ï€å‹äººæ‰"çš„"ç»ˆèº«å­¦ä¹ "è“å›¾': 'Growth: The "Lifelong Learning" Blueprint for "Ï€-Shaped Talents"',
        'åŠè½½è§‚æƒ³å°è®°ï¼šåœ¨å¤§ç†ã€åœ¨å†…è§‚ç¦…ä¿®çš„è·¯ä¸Š': 'Half-Year Mindfulness Journey in Dali',
        'ç­‘å±…æ€Â·ä¿®è¡Œï¼šæˆ‘24å²å­¦åˆ°çš„"çµé­‚è‡ªæ´½"SOP': 'Practice: The "Soul Self-Consistency" SOP I Learned at 24',
        'ç­‘å±…æ€Â·å®è·µï¼šæˆ–è®¸è®¾è®¡å®éªŒå°±æ˜¯å®¹æ˜“å¤±è´¥ï¼Œå¯¹å—ï¼Ÿ': 'Practice: Perhaps Design Experiments Tend to Fail, Right?',
        'åˆ›é€ æ€§æ€ç»´': 'Creative Thinking',
        'ç­‘å±…æ€Â·ç®—æ³•ï¼šRSSâ€”â€”åœ¨ä¿¡æ¯è¿·é›¾ä¸­æ„å»º"è®¤çŸ¥ç»¿æ´²"çš„è‰ºæœ¯': 'Algorithm: RSS - The Art of Building "Cognitive Oases" in the Information Fog',
        'åœ¨ç¦…å ‚é‡Œï¼Œæˆ‘é‡è§äº†æ‰€æœ‰äººâ€”â€”è®°ç¬¬äºŒæ¬¡å†…è§‚ç¦…ä¿®çš„ç»“ç¼˜': 'Meeting Everyone in the Meditation Hall',
        # æ–°æ ¼å¼ï¼šç­‘å±…æ€Â·åŸºå‡†
        'ç­‘å±…æ€Â·åŸºå‡†ï¼šä¸€ä»½2022å¹´çš„"çµé­‚å¿«ç…§"ï¼ˆæ™®é²æ–¯ç‰¹é—®å·ï¼‰': 'Baseline: A 2022 "Soul Snapshot" (Proust Questionnaire)',
        # æ–°æ ¼å¼ï¼šç­‘å±…æ€Â·å›å“
        'ç­‘å±…æ€Â·å›å“ï¼šæ¥è‡ª19å²çš„ç¡®è®¤â€”â€”"ä½ æˆä¸ºäº†æˆ‘æƒ³è±¡ä¸­çš„å¤§äºº"': 'Echo: Confirmation from 19-Year-Old Self - "You Became the Adult I Imagined"',
        # éƒ¨åˆ†åŒ¹é…ï¼ˆç”¨äºåŒ¹é…æ ‡é¢˜ä¸­çš„å…³é”®éƒ¨åˆ†ï¼‰
        'å¦‚æœåœ¨å¤å¤œä¸€ä¸ªæ—…äºº': 'If on a Summer Night a Traveler',
        'å¬å±±é£': 'Listening to Mountain Wind',
        'é‡æ–°è§‰å¯Ÿè‡ªæˆ‘': 'Reawakening Self-Awareness',
        'æ°¸è¿œä¸è¦åœæ­¢æƒ³è±¡': 'Never Stop Imagining',
        'ç­”æ™®é²æ–¯å…‹é—®å·': 'Answering the Proust Questionnaire',
        'å¥½æ–‡åˆ†äº«ä¸¨åœä¸‹æ¥ä¼‘æ¯ä¸€ä¸‹': 'Good Article Sharing: Stop and Rest',
        'å¦‚ä½•é¢å¯¹é‡å¤§äººç”Ÿå†³å®š': 'How to Face Major Life Decisions',
        'ä¸€ç›´æ¸¸åˆ°æµ·æ°´å˜è“': 'Swimming Till the Sea Turns Blue',
        '24å²å­¦ä¼šçš„24ä»¶äº‹': '24 Things Learned at 24',
        'æˆ–è®¸è®¾è®¡å®éªŒå°±æ˜¯å®¹æ˜“å¤±è´¥ï¼Œå¯¹å—ï¼Ÿ': 'Design Experiments Tend to Fail',
        '2025å¹´äº†ä¸ºä»€ä¹ˆæˆ‘è¿˜æ˜¯æ¨èç”¨RSSè®¢é˜…å†…å®¹': 'Why I Still Recommend RSS Subscription in 2025',
        'å’Œ19å²çš„è‡ªå·±å¯¹è¯': 'Talking to 19-Year-Old Self',
    }
    
    # å°è¯•åŒ¹é…å®Œæ•´æ ‡é¢˜ï¼ˆå¸¦å‰ç¼€ï¼‰
    if chinese_title in common_translations:
        return common_translations[chinese_title]
    if normalized_title in common_translations:
        return common_translations[normalized_title]
    
    # å°è¯•åŒ¹é…å®Œæ•´æ ‡é¢˜ï¼ˆä¸å¸¦å‰ç¼€ï¼‰
    if clean_title in common_translations:
        return common_translations[clean_title]
    if clean_normalized in common_translations:
        return common_translations[clean_normalized]
    
    # å°è¯•éƒ¨åˆ†åŒ¹é…ï¼ˆå¦‚æœæ ‡é¢˜åŒ…å«æŸä¸ªå…³é”®è¯ï¼‰
    for key, value in common_translations.items():
        normalized_key = key.replace('"', '"').replace('"', '"').replace(''', "'").replace(''', "'")
        if (key in chinese_title or key in clean_title or 
            normalized_key in normalized_title or normalized_key in clean_normalized):
            return value
    
    # å¦‚æœæœ‰åˆ†ç±»ï¼Œå°è¯•ä½¿ç”¨åˆ†ç±»ç¿»è¯‘æ ¼å¼
    # æ ¼å¼ï¼š[åˆ†ç±»]: [æ ‡é¢˜å†…å®¹ç¿»è¯‘]
    if category and category in category_translations:
        category_en = category_translations[category]
        # å°è¯•ç¿»è¯‘æ ‡é¢˜å†…å®¹éƒ¨åˆ†
        content_translation = clean_title  # é»˜è®¤ä½¿ç”¨åŸå†…å®¹
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šå†…å®¹ç¿»è¯‘é€»è¾‘
        return f"{category_en}: {content_translation}"
    
    # å¦‚æœæ— æ³•ç¿»è¯‘ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ï¼ˆéœ€è¦æ‰‹åŠ¨æ·»åŠ åˆ°æ˜ å°„è¡¨ï¼‰
    return ""


def extract_metadata_from_html(html_file: Path) -> Optional[Dict]:
    """ä»HTMLæ–‡ä»¶ä¸­æå–å…ƒæ•°æ®"""
    try:
        with open(html_file, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        # æå–titleæ ‡ç­¾
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)
        if not title_match:
            return None
        
        title = title_match.group(1).strip()
        # ç§»é™¤" - ç­‘å±…æ€"åç¼€
        title = title.replace(' - ç­‘å±…æ€', '').strip()
        title = html.unescape(title)
        
        # æå–description metaæ ‡ç­¾
        desc_match = re.search(
            r'<meta\s+name=["\']description["\']\s+content=["\'](.*?)["\']',
            html_content,
            re.IGNORECASE
        )
        description = desc_match.group(1) if desc_match else ''
        description = html.unescape(description)
        
        # æå–keywords metaæ ‡ç­¾
        keywords_match = re.search(
            r'<meta\s+name=["\']keywords["\']\s+content=["\'](.*?)["\']',
            html_content,
            re.IGNORECASE
        )
        keywords_str = keywords_match.group(1) if keywords_match else ''
        keywords_str = html.unescape(keywords_str)
        
        # è§£æå…³é”®è¯åˆ—è¡¨ï¼ˆç§»é™¤"ç­‘å±…æ€"å¦‚æœå­˜åœ¨ï¼‰
        keywords = [k.strip() for k in keywords_str.split(',') if k.strip()]
        keywords = [k for k in keywords if k != 'ç­‘å±…æ€']
        
        return {
            'title': title,
            'description': description,
            'keywords': keywords
        }
    except Exception as e:
        print(f"  é”™è¯¯: è¯»å–HTMLæ–‡ä»¶å¤±è´¥: {e}")
        return None


def escape_html(text: str) -> str:
    """è½¬ä¹‰HTMLç‰¹æ®Šå­—ç¬¦"""
    return html.escape(text, quote=True)


def update_html_metadata(html_file: Path, metadata: dict, translation_mapping: Dict[str, str]) -> bool:
    """æ›´æ–°HTMLæ–‡ä»¶ä¸­çš„å…ƒæ•°æ®"""
    try:
        with open(html_file, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        title = metadata['title']
        description = metadata['description']
        keywords = metadata['keywords']
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­
        is_excluded = any(excluded in title or excluded in html_file.name for excluded in EXCLUDED_BLOGS)
        
        # ç”Ÿæˆè‹±æ–‡æ ‡é¢˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
        english_title = ""
        if not is_excluded:
            english_title = translate_chinese_title(title, translation_mapping)
            if english_title:
                # ä¿å­˜åˆ°æ˜ å°„è¡¨
                translation_mapping[title] = english_title
                print(f"  ğŸ“ è‹±æ–‡æ ‡é¢˜: {english_title}")
            else:
                print(f"  âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°è‹±æ–‡æ ‡é¢˜ç¿»è¯‘ï¼Œè¯·æ‰‹åŠ¨æ·»åŠ åˆ°æ˜ å°„è¡¨")
        
        # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
        title_escaped = escape_html(title)
        desc_escaped = escape_html(description)
        
        # æ›´æ–°titleæ ‡ç­¾
        title_pattern = r'<title[^>]*>.*?</title>'
        new_title = f'<title>{title_escaped} - ç­‘å±…æ€</title>'
        html_content = re.sub(title_pattern, new_title, html_content, flags=re.IGNORECASE | re.DOTALL)
        
        # æ›´æ–°description metaæ ‡ç­¾
        desc_pattern = r'<meta\s+name=["\']description["\']\s+content=["\'][^"\']*["\']\s*/?>'
        new_desc = f'<meta name="description" content="{desc_escaped}">'
        if re.search(desc_pattern, html_content, re.IGNORECASE):
            html_content = re.sub(desc_pattern, new_desc, html_content, flags=re.IGNORECASE)
        else:
            # å¦‚æœæ²¡æœ‰descriptionæ ‡ç­¾ï¼Œåœ¨titleåé¢æ·»åŠ 
            html_content = re.sub(
                r'(<title[^>]*>.*?</title>)',
                r'\1\n    ' + new_desc,
                html_content,
                flags=re.IGNORECASE | re.DOTALL
            )
        
        # æ›´æ–°keywords metaæ ‡ç­¾
        keywords_list = keywords.copy()
        if 'ç­‘å±…æ€' not in keywords_list:
            keywords_list.append('ç­‘å±…æ€')
        keywords_str = ', '.join(keywords_list)
        keywords_escaped = escape_html(keywords_str)
        
        keywords_pattern = r'<meta\s+name=["\']keywords["\']\s+content=["\'][^"\']*["\']\s*/?>'
        new_keywords = f'<meta name="keywords" content="{keywords_escaped}">'
        if re.search(keywords_pattern, html_content, re.IGNORECASE):
            html_content = re.sub(keywords_pattern, new_keywords, html_content, flags=re.IGNORECASE)
        else:
            # å¦‚æœæ²¡æœ‰keywordsæ ‡ç­¾ï¼Œåœ¨descriptionåé¢æ·»åŠ 
            html_content = re.sub(
                r'(<meta\s+name=["\']description["\']\s+content=["\'][^"\']*["\']\s*/?>)',
                r'\1\n    ' + new_keywords,
                html_content,
                flags=re.IGNORECASE
            )
        
        # æ›´æ–°Open Graphå’ŒTwitterå¡ç‰‡
        og_title_pattern = r'<meta\s+property=["\']og:title["\']\s+content=["\'][^"\']*["\']\s*/?>'
        og_desc_pattern = r'<meta\s+property=["\']og:description["\']\s+content=["\'][^"\']*["\']\s*/?>'
        og_url_pattern = r'<meta\s+property=["\']og:url["\']\s+content=["\'][^"\']*["\']\s*/?>'
        og_type_pattern = r'<meta\s+property=["\']og:type["\']\s+content=["\'][^"\']*["\']\s*/?>'
        twitter_title_pattern = r'<meta\s+name=["\']twitter:title["\']\s+content=["\'][^"\']*["\']\s*/?>'
        twitter_desc_pattern = r'<meta\s+name=["\']twitter:description["\']\s+content=["\'][^"\']*["\']\s*/?>'
        twitter_card_pattern = r'<meta\s+name=["\']twitter:card["\']\s+content=["\'][^"\']*["\']\s*/?>'
        
        # æå–URL
        url_match = re.search(r'<link\s+rel=["\']canonical["\']\s+href=["\']([^"\']+)["\']', html_content, re.IGNORECASE)
        if not url_match:
            # ä»æ–‡ä»¶åç”ŸæˆURL
            url = f"https://thinkingleaf.space/blogs/{html_file.name}"
        else:
            url = url_match.group(1)
        
        new_og_title = f'<meta property="og:title" content="{title_escaped} - ç­‘å±…æ€">'
        new_og_desc = f'<meta property="og:description" content="{desc_escaped}">'
        new_og_url = f'<meta property="og:url" content="{url}">'
        new_og_type = '<meta property="og:type" content="article">'
        new_twitter_title = f'<meta name="twitter:title" content="{title_escaped} - ç­‘å±…æ€">'
        new_twitter_desc = f'<meta name="twitter:description" content="{desc_escaped}">'
        new_twitter_card = '<meta name="twitter:card" content="summary">'
        
        # æ›´æ–°æˆ–æ·»åŠ Open Graphæ ‡ç­¾
        if re.search(og_title_pattern, html_content, re.IGNORECASE):
            html_content = re.sub(og_title_pattern, new_og_title, html_content, flags=re.IGNORECASE)
        else:
            # åœ¨keywordsåé¢æ·»åŠ 
            html_content = re.sub(
                r'(<meta\s+name=["\']keywords["\']\s+content=["\'][^"\']*["\']\s*/?>)',
                r'\1\n    ' + new_og_type + '\n    ' + new_og_url + '\n    ' + new_og_title + '\n    ' + new_og_desc,
                html_content,
                flags=re.IGNORECASE,
                count=1
            )
        
        if re.search(og_desc_pattern, html_content, re.IGNORECASE):
            html_content = re.sub(og_desc_pattern, new_og_desc, html_content, flags=re.IGNORECASE)
        
        # æ›´æ–°æˆ–æ·»åŠ Twitteræ ‡ç­¾
        if re.search(twitter_title_pattern, html_content, re.IGNORECASE):
            html_content = re.sub(twitter_title_pattern, new_twitter_title, html_content, flags=re.IGNORECASE)
        else:
            # åœ¨og:descriptionåé¢æ·»åŠ 
            html_content = re.sub(
                r'(<meta\s+property=["\']og:description["\']\s+content=["\'][^"\']*["\']\s*/?>)',
                r'\1\n    ' + new_twitter_card + '\n    ' + new_twitter_title + '\n    ' + new_twitter_desc,
                html_content,
                flags=re.IGNORECASE,
                count=1
            )
        
        if re.search(twitter_desc_pattern, html_content, re.IGNORECASE):
            html_content = re.sub(twitter_desc_pattern, new_twitter_desc, html_content, flags=re.IGNORECASE)
        
        # æ›´æ–°h1æ ‡é¢˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        # å°è¯•åŒ¹é…å¸¦class="post-title"çš„h1
        h1_pattern = r'<h1[^>]*class=["\']post-title["\'][^>]*>.*?</h1>'
        h1_match = re.search(h1_pattern, html_content, re.IGNORECASE | re.DOTALL)
        if h1_match:
            new_h1 = f'<h1 class="post-title">{title_escaped}</h1>'
            html_content = re.sub(h1_pattern, new_h1, html_content, flags=re.IGNORECASE | re.DOTALL)
        else:
            # å°è¯•åŒ¹é…åœ¨post-headerå†…çš„h1
            header_h1_pattern = r'(<header[^>]*class=["\']post-header["\'][^>]*>.*?<h1[^>]*>).*?(</h1>)'
            header_h1_match = re.search(header_h1_pattern, html_content, re.IGNORECASE | re.DOTALL)
            if header_h1_match:
                new_h1 = f'{header_h1_match.group(1)}{title_escaped}{header_h1_match.group(2)}'
                html_content = re.sub(header_h1_pattern, lambda m: f'{m.group(1)}{title_escaped}{m.group(2)}', html_content, flags=re.IGNORECASE | re.DOTALL)
            else:
                # å°è¯•åŒ¹é…class="page-title"çš„h1
                page_h1_pattern = r'<h1[^>]*class=["\']page-title["\'][^>]*>.*?</h1>'
                page_h1_match = re.search(page_h1_pattern, html_content, re.IGNORECASE | re.DOTALL)
                if page_h1_match:
                    new_h1 = f'<h1 class="page-title">{title_escaped}</h1>'
                    html_content = re.sub(page_h1_pattern, new_h1, html_content, flags=re.IGNORECASE | re.DOTALL)
        
        # å†™å›æ–‡ä»¶
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return True
    except Exception as e:
        print(f"  é”™è¯¯: æ›´æ–°HTMLæ–‡ä»¶å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("åšå®¢å…ƒæ•°æ®åŒæ­¥å·¥å…·")
    print("=" * 60)
    print()
    
    if not BLOGS_DIR.exists():
        print(f"âŒ é”™è¯¯: åšå®¢ç›®å½•ä¸å­˜åœ¨: {BLOGS_DIR}")
        return
    
    # åŠ è½½ç¿»è¯‘æ˜ å°„è¡¨
    translation_mapping = load_title_translation()
    
    # è·å–æ‰€æœ‰HTMLæ–‡ä»¶
    html_files = sorted(BLOGS_DIR.glob('*.html'))
    
    if not html_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•åšå®¢HTMLæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(html_files)} ä¸ªåšå®¢æ–‡ä»¶")
    print()
    
    success_count = 0
    skip_count = 0
    failed_count = 0
    
    for html_file in html_files:
        print(f"å¤„ç†: {html_file.name}")
        print("-" * 60)
        
        # æå–å…ƒæ•°æ®
        metadata = extract_metadata_from_html(html_file)
        if not metadata or not metadata.get('title'):
            print("  è·³è¿‡ï¼ˆæ— æ³•æå–å…ƒæ•°æ®ï¼‰")
            skip_count += 1
            print()
            continue
        
        title = metadata['title']
        description = metadata['description']
        keywords = metadata['keywords']
        
        print(f"  æ ‡é¢˜: {title}")
        print(f"  ç®€ä»‹: {description[:80]}..." if len(description) > 80 else f"  ç®€ä»‹: {description}")
        print(f"  å…³é”®è¯: {', '.join(keywords[:8])}..." if len(keywords) > 8 else f"  å…³é”®è¯: {', '.join(keywords)}")
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­
        is_excluded = any(excluded in title or excluded in html_file.name for excluded in EXCLUDED_BLOGS)
        if is_excluded:
            print(f"  â­ï¸  è·³è¿‡ï¼ˆåœ¨æ’é™¤åˆ—è¡¨ä¸­ï¼Œä¸ç”Ÿæˆè‹±æ–‡æ ‡é¢˜ï¼‰")
        
        # æ›´æ–°HTMLæ–‡ä»¶
        if update_html_metadata(html_file, metadata, translation_mapping):
            success_count += 1
            print("  âœ… æˆåŠŸåŒæ­¥")
        else:
            failed_count += 1
            print("  âŒ åŒæ­¥å¤±è´¥")
        
        print()
    
    # ä¿å­˜ç¿»è¯‘æ˜ å°„è¡¨
    save_title_translation(translation_mapping)
    
    print("=" * 60)
    print(f"åŒæ­¥å®Œæˆï¼")
    print(f"  æˆåŠŸ: {success_count}")
    print(f"  è·³è¿‡: {skip_count}")
    print(f"  å¤±è´¥: {failed_count}")
    print("=" * 60)
    
    if translation_mapping:
        print(f"\nğŸ’¾ å·²ä¿å­˜ç¿»è¯‘æ˜ å°„åˆ°: {TITLE_TRANSLATION_FILE}")


if __name__ == '__main__':
    main()

